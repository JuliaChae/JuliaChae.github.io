<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>projects | Julia Chae</title> <meta name="author" content="Julia Chae"> <meta name="description" content="A growing collection of your cool projects."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://juliachae.github.io/projects/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://JuliaChae.github.io/"> <span class="font-weight-bold">Julia</span> Chae </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="#about"> about </a> </li> <li class="nav-item "> <a class="nav-link" href="#education"> education </a> </li> <li class="nav-item "> <a class="nav-link" href="#publications"> publications </a> </li> <li class="nav-item "> <a class="nav-link" href="#projects"> projects </a> </li> <li class="nav-item "> <a class="nav-link" href="#misc"> interests </a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">projects</h1> <p class="post-description">A growing collection of your cool projects.</p> </header> <article> <div class="projects"> <h2 class="category">work</h2> <div class="grid"> <h2 style="padding-top: 30px;"><b>projects</b></h2> <p>For more projects, please visit my <a href="https://github.com/JuliaChae" rel="external nofollow noopener" target="_blank">Github</a>&lt;/b&gt;</p> <table style="padding-top: 10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/carla_imi.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Unsupervised Multimodal Representation Learning for Robot Controls</strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>. With Yewon L., Pranit C. <br> <br> Investigated how unsupervised representation learning could be utilized in an imitation learning (IL) task such as robot path following. In this project, we successfully trained a multimodal representation from unlabelled path traversal data, fine-tuned on minimal labeled data.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/multiview.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Analysis of Monocular 3D Object Detection Network Performance on Multiview Datasets</strong> <br> [Research Presentation] @ <strong><a href="https://www.trailab.utias.utoronto.ca/" rel="external nofollow noopener" target="_blank">TRAILab</a></strong>, funded by <strong>NSERC USRA</strong> <br> <a href="https://github.com/JuliaChae/M3D-RPN-Waymo" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Designed and executed an analysis of effect of camera perspectives on object detection networks, identifying limitations of multi-view self-driving datasets and current state-of-art monocular networks. The performance of trained monocular networks on various views of multiview dataset was analyzed with respect to multiple factors including distance, rotation and level of occlusion.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/pointfusion.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Adversarial Examples for Multimodal Object Detection Networks </strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>, funded by <strong>ESROP Dr Allison Mackay Award</strong> <br> <a href="https://github.com/JuliaChae/Pointfusion" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Led the implementation of multi-sensor object detection neural networks to state-of-art performance using PyTorch for an adversarial examples project, to improve robustness of image and lidar perception models. Trained fasterRCNN on nuScenes self driving car dataset and implemented Pointfusion architecture based on the original paper.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/RSX.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Rover Drive System</strong> <br> [Competition] University/European/Canadian Rover Challenges <br> <a href="https://github.com/rsx-utoronto/rover/tree/develop" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Spearheaded development of rover drive control software pipeline which includes joystick integration, configuration of I2C protocol for motor drivers, variable speed control and AR tag detection. Languages used were Python, C++ and ROS was utilized to handle communication between topics.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/rob301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Robot Mail Delivery Localization System</strong> <br>[Final Project], University of Toronto, ROB301 Introduction to Robotics. With Samantha U. <br> <a href="assets/pdf/ROB301_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> A PID control system, bayesian localization and state measurement models were developed in order to enable the robot to navigate an unknown hallway to deliver mail at specified destinations. The project was completed virtually using the Turtlebot 3 Waffle Pi robot simulated in a Gazebo environment. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/aps360.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Accent Classification Network</strong> <br>[Final Project], University of Toronto, APS360 Applied Fundamentals of Machine Learning. With Mingshi C., Catherine G., Rocco R. <br> <a href="assets/pdf/APS360_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> Motivated by challenges experienced with voice-controlled devices, our team developed a neural network speech accent classifier which takes an accented English phrase as input and correctly identifies the origin of that speaker’s accent. The network developed was a CRNN architecture with mixed convolutional and recurrent neural network layers which learned both temporal and characteristic features of the audio data. The audio was processed into Mel-frequency cepstral coefficients (MFCC) features for input. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/ESC301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Electric Car Charger Robot </strong> <br>[Final Project], University of Toronto, ESC301 Praxis III. With Daniel R. and Chan Y. <br> <a href="assets/pdf/ESC301_Project%20Proposal%20Report.pdf">Detailed Design Report</a> <br> <br> Designed, conceptualized and prototyped an autonomous robot which locates and traverses to a charging port on a car to plug in the charger, utilizing time-of-flight sensor, color sensor, and pi-camera. Developed a modular program in Python and C++, each responsible for sensor or chassis control; utilized embedded programing</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Praxis2.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Sourdough IR Fermentometer Autonomous Robot </strong> <br> [Final Project], University of Toronto, ESC102 Praxis II. With Mingshi C., Daniela L., Peter S. <br> <a href="assets/pdf/ShowcasePoster.pdf">Poster</a> / <a href="assets/img/praxis2_demo.mp4">Demo Video</a> <br> <br> Designed a fully automated robotics system to inform sourdough bakers when their dough is ready to be put into the oven, in collaboration with local Toronto bakers at Forno Cultura and Blackbird Bakeries. The system develops a custom yeast-growth model and while the dough is expanding, the robot uses a 2 axis linear stage, time of flight sensor and temperature sensor to track the dough growth and alerts the bakers when it is ready. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Chess.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Chess AI</strong> <br> [Class Project], University of Toronto, CSC190 Algorithms and Data Structures. <br> <a href="https://github.com/JuliaChae/Chess-AI" rel="external nofollow noopener" target="_blank">Code</a> <br> <br>Developed an automatic chess player that while utilizes alpha-beta pruning on a min-max game tree in order to select the best move against a human opponent. </p1> </td> </tr> </tbody></table> &lt;/tbody&gt;&lt;/table&gt; <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Gameboy.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Original Gameboy Project: Bit's Adventures</strong> <br> [Final Project], University of Toronto, MIE438 Microcontrollers and Embedded Processors. <br> <a href="https://github.com/Maelstrum127/MIE438-GameBoy-Project" rel="external nofollow noopener" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=2vMHTOsuGYA&amp;ab_channel=CatGlossop" rel="external nofollow noopener" target="_blank">Demo Video</a> <br> <br> Bit’s Adventure is an original interactive RPG for the Original Game Boy which was coded using gbdk package in C. The game can be played on Game Boy emulators such as mGBA. </p1> </td> </tr> </tbody></table> <h2 style="padding-top: 30px;"><b>projects</b></h2> <p>For more projects, please visit my <a href="https://github.com/JuliaChae" rel="external nofollow noopener" target="_blank">Github</a>&lt;/b&gt;</p> <table style="padding-top: 10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/carla_imi.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Unsupervised Multimodal Representation Learning for Robot Controls</strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>. With Yewon L., Pranit C. <br> <br> Investigated how unsupervised representation learning could be utilized in an imitation learning (IL) task such as robot path following. In this project, we successfully trained a multimodal representation from unlabelled path traversal data, fine-tuned on minimal labeled data.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/multiview.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Analysis of Monocular 3D Object Detection Network Performance on Multiview Datasets</strong> <br> [Research Presentation] @ <strong><a href="https://www.trailab.utias.utoronto.ca/" rel="external nofollow noopener" target="_blank">TRAILab</a></strong>, funded by <strong>NSERC USRA</strong> <br> <a href="https://github.com/JuliaChae/M3D-RPN-Waymo" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Designed and executed an analysis of effect of camera perspectives on object detection networks, identifying limitations of multi-view self-driving datasets and current state-of-art monocular networks. The performance of trained monocular networks on various views of multiview dataset was analyzed with respect to multiple factors including distance, rotation and level of occlusion.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/pointfusion.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Adversarial Examples for Multimodal Object Detection Networks </strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>, funded by <strong>ESROP Dr Allison Mackay Award</strong> <br> <a href="https://github.com/JuliaChae/Pointfusion" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Led the implementation of multi-sensor object detection neural networks to state-of-art performance using PyTorch for an adversarial examples project, to improve robustness of image and lidar perception models. Trained fasterRCNN on nuScenes self driving car dataset and implemented Pointfusion architecture based on the original paper.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/RSX.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Rover Drive System</strong> <br> [Competition] University/European/Canadian Rover Challenges <br> <a href="https://github.com/rsx-utoronto/rover/tree/develop" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Spearheaded development of rover drive control software pipeline which includes joystick integration, configuration of I2C protocol for motor drivers, variable speed control and AR tag detection. Languages used were Python, C++ and ROS was utilized to handle communication between topics.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/rob301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Robot Mail Delivery Localization System</strong> <br>[Final Project], University of Toronto, ROB301 Introduction to Robotics. With Samantha U. <br> <a href="assets/pdf/ROB301_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> A PID control system, bayesian localization and state measurement models were developed in order to enable the robot to navigate an unknown hallway to deliver mail at specified destinations. The project was completed virtually using the Turtlebot 3 Waffle Pi robot simulated in a Gazebo environment. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/aps360.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Accent Classification Network</strong> <br>[Final Project], University of Toronto, APS360 Applied Fundamentals of Machine Learning. With Mingshi C., Catherine G., Rocco R. <br> <a href="assets/pdf/APS360_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> Motivated by challenges experienced with voice-controlled devices, our team developed a neural network speech accent classifier which takes an accented English phrase as input and correctly identifies the origin of that speaker’s accent. The network developed was a CRNN architecture with mixed convolutional and recurrent neural network layers which learned both temporal and characteristic features of the audio data. The audio was processed into Mel-frequency cepstral coefficients (MFCC) features for input. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/ESC301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Electric Car Charger Robot </strong> <br>[Final Project], University of Toronto, ESC301 Praxis III. With Daniel R. and Chan Y. <br> <a href="assets/pdf/ESC301_Project%20Proposal%20Report.pdf">Detailed Design Report</a> <br> <br> Designed, conceptualized and prototyped an autonomous robot which locates and traverses to a charging port on a car to plug in the charger, utilizing time-of-flight sensor, color sensor, and pi-camera. Developed a modular program in Python and C++, each responsible for sensor or chassis control; utilized embedded programing</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Praxis2.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Sourdough IR Fermentometer Autonomous Robot </strong> <br> [Final Project], University of Toronto, ESC102 Praxis II. With Mingshi C., Daniela L., Peter S. <br> <a href="assets/pdf/ShowcasePoster.pdf">Poster</a> / <a href="assets/img/praxis2_demo.mp4">Demo Video</a> <br> <br> Designed a fully automated robotics system to inform sourdough bakers when their dough is ready to be put into the oven, in collaboration with local Toronto bakers at Forno Cultura and Blackbird Bakeries. The system develops a custom yeast-growth model and while the dough is expanding, the robot uses a 2 axis linear stage, time of flight sensor and temperature sensor to track the dough growth and alerts the bakers when it is ready. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Chess.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Chess AI</strong> <br> [Class Project], University of Toronto, CSC190 Algorithms and Data Structures. <br> <a href="https://github.com/JuliaChae/Chess-AI" rel="external nofollow noopener" target="_blank">Code</a> <br> <br>Developed an automatic chess player that while utilizes alpha-beta pruning on a min-max game tree in order to select the best move against a human opponent. </p1> </td> </tr> </tbody></table> &lt;/tbody&gt;&lt;/table&gt; <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Gameboy.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Original Gameboy Project: Bit's Adventures</strong> <br> [Final Project], University of Toronto, MIE438 Microcontrollers and Embedded Processors. <br> <a href="https://github.com/Maelstrum127/MIE438-GameBoy-Project" rel="external nofollow noopener" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=2vMHTOsuGYA&amp;ab_channel=CatGlossop" rel="external nofollow noopener" target="_blank">Demo Video</a> <br> <br> Bit’s Adventure is an original interactive RPG for the Original Game Boy which was coded using gbdk package in C. The game can be played on Game Boy emulators such as mGBA. </p1> </td> </tr> </tbody></table> <h2 style="padding-top: 30px;"><b>projects</b></h2> <p>For more projects, please visit my <a href="https://github.com/JuliaChae" rel="external nofollow noopener" target="_blank">Github</a>&lt;/b&gt;</p> <table style="padding-top: 10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/carla_imi.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Unsupervised Multimodal Representation Learning for Robot Controls</strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>. With Yewon L., Pranit C. <br> <br> Investigated how unsupervised representation learning could be utilized in an imitation learning (IL) task such as robot path following. In this project, we successfully trained a multimodal representation from unlabelled path traversal data, fine-tuned on minimal labeled data.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/multiview.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Analysis of Monocular 3D Object Detection Network Performance on Multiview Datasets</strong> <br> [Research Presentation] @ <strong><a href="https://www.trailab.utias.utoronto.ca/" rel="external nofollow noopener" target="_blank">TRAILab</a></strong>, funded by <strong>NSERC USRA</strong> <br> <a href="https://github.com/JuliaChae/M3D-RPN-Waymo" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Designed and executed an analysis of effect of camera perspectives on object detection networks, identifying limitations of multi-view self-driving datasets and current state-of-art monocular networks. The performance of trained monocular networks on various views of multiview dataset was analyzed with respect to multiple factors including distance, rotation and level of occlusion.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/pointfusion.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Adversarial Examples for Multimodal Object Detection Networks </strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>, funded by <strong>ESROP Dr Allison Mackay Award</strong> <br> <a href="https://github.com/JuliaChae/Pointfusion" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Led the implementation of multi-sensor object detection neural networks to state-of-art performance using PyTorch for an adversarial examples project, to improve robustness of image and lidar perception models. Trained fasterRCNN on nuScenes self driving car dataset and implemented Pointfusion architecture based on the original paper.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/RSX.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Rover Drive System</strong> <br> [Competition] University/European/Canadian Rover Challenges <br> <a href="https://github.com/rsx-utoronto/rover/tree/develop" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Spearheaded development of rover drive control software pipeline which includes joystick integration, configuration of I2C protocol for motor drivers, variable speed control and AR tag detection. Languages used were Python, C++ and ROS was utilized to handle communication between topics.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/rob301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Robot Mail Delivery Localization System</strong> <br>[Final Project], University of Toronto, ROB301 Introduction to Robotics. With Samantha U. <br> <a href="assets/pdf/ROB301_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> A PID control system, bayesian localization and state measurement models were developed in order to enable the robot to navigate an unknown hallway to deliver mail at specified destinations. The project was completed virtually using the Turtlebot 3 Waffle Pi robot simulated in a Gazebo environment. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/aps360.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Accent Classification Network</strong> <br>[Final Project], University of Toronto, APS360 Applied Fundamentals of Machine Learning. With Mingshi C., Catherine G., Rocco R. <br> <a href="assets/pdf/APS360_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> Motivated by challenges experienced with voice-controlled devices, our team developed a neural network speech accent classifier which takes an accented English phrase as input and correctly identifies the origin of that speaker’s accent. The network developed was a CRNN architecture with mixed convolutional and recurrent neural network layers which learned both temporal and characteristic features of the audio data. The audio was processed into Mel-frequency cepstral coefficients (MFCC) features for input. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/ESC301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Electric Car Charger Robot </strong> <br>[Final Project], University of Toronto, ESC301 Praxis III. With Daniel R. and Chan Y. <br> <a href="assets/pdf/ESC301_Project%20Proposal%20Report.pdf">Detailed Design Report</a> <br> <br> Designed, conceptualized and prototyped an autonomous robot which locates and traverses to a charging port on a car to plug in the charger, utilizing time-of-flight sensor, color sensor, and pi-camera. Developed a modular program in Python and C++, each responsible for sensor or chassis control; utilized embedded programing</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Praxis2.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Sourdough IR Fermentometer Autonomous Robot </strong> <br> [Final Project], University of Toronto, ESC102 Praxis II. With Mingshi C., Daniela L., Peter S. <br> <a href="assets/pdf/ShowcasePoster.pdf">Poster</a> / <a href="assets/img/praxis2_demo.mp4">Demo Video</a> <br> <br> Designed a fully automated robotics system to inform sourdough bakers when their dough is ready to be put into the oven, in collaboration with local Toronto bakers at Forno Cultura and Blackbird Bakeries. The system develops a custom yeast-growth model and while the dough is expanding, the robot uses a 2 axis linear stage, time of flight sensor and temperature sensor to track the dough growth and alerts the bakers when it is ready. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Chess.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Chess AI</strong> <br> [Class Project], University of Toronto, CSC190 Algorithms and Data Structures. <br> <a href="https://github.com/JuliaChae/Chess-AI" rel="external nofollow noopener" target="_blank">Code</a> <br> <br>Developed an automatic chess player that while utilizes alpha-beta pruning on a min-max game tree in order to select the best move against a human opponent. </p1> </td> </tr> </tbody></table> &lt;/tbody&gt;&lt;/table&gt; <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Gameboy.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Original Gameboy Project: Bit's Adventures</strong> <br> [Final Project], University of Toronto, MIE438 Microcontrollers and Embedded Processors. <br> <a href="https://github.com/Maelstrum127/MIE438-GameBoy-Project" rel="external nofollow noopener" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=2vMHTOsuGYA&amp;ab_channel=CatGlossop" rel="external nofollow noopener" target="_blank">Demo Video</a> <br> <br> Bit’s Adventure is an original interactive RPG for the Original Game Boy which was coded using gbdk package in C. The game can be played on Game Boy emulators such as mGBA. </p1> </td> </tr> </tbody></table> </div> <h2 class="category">fun</h2> <div class="grid"> <h2 style="padding-top: 30px;"><b>projects</b></h2> <p>For more projects, please visit my <a href="https://github.com/JuliaChae" rel="external nofollow noopener" target="_blank">Github</a>&lt;/b&gt;</p> <table style="padding-top: 10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/carla_imi.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Unsupervised Multimodal Representation Learning for Robot Controls</strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>. With Yewon L., Pranit C. <br> <br> Investigated how unsupervised representation learning could be utilized in an imitation learning (IL) task such as robot path following. In this project, we successfully trained a multimodal representation from unlabelled path traversal data, fine-tuned on minimal labeled data.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/multiview.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Analysis of Monocular 3D Object Detection Network Performance on Multiview Datasets</strong> <br> [Research Presentation] @ <strong><a href="https://www.trailab.utias.utoronto.ca/" rel="external nofollow noopener" target="_blank">TRAILab</a></strong>, funded by <strong>NSERC USRA</strong> <br> <a href="https://github.com/JuliaChae/M3D-RPN-Waymo" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Designed and executed an analysis of effect of camera perspectives on object detection networks, identifying limitations of multi-view self-driving datasets and current state-of-art monocular networks. The performance of trained monocular networks on various views of multiview dataset was analyzed with respect to multiple factors including distance, rotation and level of occlusion.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/pointfusion.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Adversarial Examples for Multimodal Object Detection Networks </strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>, funded by <strong>ESROP Dr Allison Mackay Award</strong> <br> <a href="https://github.com/JuliaChae/Pointfusion" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Led the implementation of multi-sensor object detection neural networks to state-of-art performance using PyTorch for an adversarial examples project, to improve robustness of image and lidar perception models. Trained fasterRCNN on nuScenes self driving car dataset and implemented Pointfusion architecture based on the original paper.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/RSX.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Rover Drive System</strong> <br> [Competition] University/European/Canadian Rover Challenges <br> <a href="https://github.com/rsx-utoronto/rover/tree/develop" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Spearheaded development of rover drive control software pipeline which includes joystick integration, configuration of I2C protocol for motor drivers, variable speed control and AR tag detection. Languages used were Python, C++ and ROS was utilized to handle communication between topics.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/rob301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Robot Mail Delivery Localization System</strong> <br>[Final Project], University of Toronto, ROB301 Introduction to Robotics. With Samantha U. <br> <a href="assets/pdf/ROB301_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> A PID control system, bayesian localization and state measurement models were developed in order to enable the robot to navigate an unknown hallway to deliver mail at specified destinations. The project was completed virtually using the Turtlebot 3 Waffle Pi robot simulated in a Gazebo environment. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/aps360.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Accent Classification Network</strong> <br>[Final Project], University of Toronto, APS360 Applied Fundamentals of Machine Learning. With Mingshi C., Catherine G., Rocco R. <br> <a href="assets/pdf/APS360_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> Motivated by challenges experienced with voice-controlled devices, our team developed a neural network speech accent classifier which takes an accented English phrase as input and correctly identifies the origin of that speaker’s accent. The network developed was a CRNN architecture with mixed convolutional and recurrent neural network layers which learned both temporal and characteristic features of the audio data. The audio was processed into Mel-frequency cepstral coefficients (MFCC) features for input. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/ESC301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Electric Car Charger Robot </strong> <br>[Final Project], University of Toronto, ESC301 Praxis III. With Daniel R. and Chan Y. <br> <a href="assets/pdf/ESC301_Project%20Proposal%20Report.pdf">Detailed Design Report</a> <br> <br> Designed, conceptualized and prototyped an autonomous robot which locates and traverses to a charging port on a car to plug in the charger, utilizing time-of-flight sensor, color sensor, and pi-camera. Developed a modular program in Python and C++, each responsible for sensor or chassis control; utilized embedded programing</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Praxis2.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Sourdough IR Fermentometer Autonomous Robot </strong> <br> [Final Project], University of Toronto, ESC102 Praxis II. With Mingshi C., Daniela L., Peter S. <br> <a href="assets/pdf/ShowcasePoster.pdf">Poster</a> / <a href="assets/img/praxis2_demo.mp4">Demo Video</a> <br> <br> Designed a fully automated robotics system to inform sourdough bakers when their dough is ready to be put into the oven, in collaboration with local Toronto bakers at Forno Cultura and Blackbird Bakeries. The system develops a custom yeast-growth model and while the dough is expanding, the robot uses a 2 axis linear stage, time of flight sensor and temperature sensor to track the dough growth and alerts the bakers when it is ready. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Chess.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Chess AI</strong> <br> [Class Project], University of Toronto, CSC190 Algorithms and Data Structures. <br> <a href="https://github.com/JuliaChae/Chess-AI" rel="external nofollow noopener" target="_blank">Code</a> <br> <br>Developed an automatic chess player that while utilizes alpha-beta pruning on a min-max game tree in order to select the best move against a human opponent. </p1> </td> </tr> </tbody></table> &lt;/tbody&gt;&lt;/table&gt; <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Gameboy.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Original Gameboy Project: Bit's Adventures</strong> <br> [Final Project], University of Toronto, MIE438 Microcontrollers and Embedded Processors. <br> <a href="https://github.com/Maelstrum127/MIE438-GameBoy-Project" rel="external nofollow noopener" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=2vMHTOsuGYA&amp;ab_channel=CatGlossop" rel="external nofollow noopener" target="_blank">Demo Video</a> <br> <br> Bit’s Adventure is an original interactive RPG for the Original Game Boy which was coded using gbdk package in C. The game can be played on Game Boy emulators such as mGBA. </p1> </td> </tr> </tbody></table> <h2 style="padding-top: 30px;"><b>projects</b></h2> <p>For more projects, please visit my <a href="https://github.com/JuliaChae" rel="external nofollow noopener" target="_blank">Github</a>&lt;/b&gt;</p> <table style="padding-top: 10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/carla_imi.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Unsupervised Multimodal Representation Learning for Robot Controls</strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>. With Yewon L., Pranit C. <br> <br> Investigated how unsupervised representation learning could be utilized in an imitation learning (IL) task such as robot path following. In this project, we successfully trained a multimodal representation from unlabelled path traversal data, fine-tuned on minimal labeled data.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/multiview.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Analysis of Monocular 3D Object Detection Network Performance on Multiview Datasets</strong> <br> [Research Presentation] @ <strong><a href="https://www.trailab.utias.utoronto.ca/" rel="external nofollow noopener" target="_blank">TRAILab</a></strong>, funded by <strong>NSERC USRA</strong> <br> <a href="https://github.com/JuliaChae/M3D-RPN-Waymo" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Designed and executed an analysis of effect of camera perspectives on object detection networks, identifying limitations of multi-view self-driving datasets and current state-of-art monocular networks. The performance of trained monocular networks on various views of multiview dataset was analyzed with respect to multiple factors including distance, rotation and level of occlusion.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/pointfusion.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Adversarial Examples for Multimodal Object Detection Networks </strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>, funded by <strong>ESROP Dr Allison Mackay Award</strong> <br> <a href="https://github.com/JuliaChae/Pointfusion" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Led the implementation of multi-sensor object detection neural networks to state-of-art performance using PyTorch for an adversarial examples project, to improve robustness of image and lidar perception models. Trained fasterRCNN on nuScenes self driving car dataset and implemented Pointfusion architecture based on the original paper.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/RSX.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Rover Drive System</strong> <br> [Competition] University/European/Canadian Rover Challenges <br> <a href="https://github.com/rsx-utoronto/rover/tree/develop" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Spearheaded development of rover drive control software pipeline which includes joystick integration, configuration of I2C protocol for motor drivers, variable speed control and AR tag detection. Languages used were Python, C++ and ROS was utilized to handle communication between topics.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/rob301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Robot Mail Delivery Localization System</strong> <br>[Final Project], University of Toronto, ROB301 Introduction to Robotics. With Samantha U. <br> <a href="assets/pdf/ROB301_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> A PID control system, bayesian localization and state measurement models were developed in order to enable the robot to navigate an unknown hallway to deliver mail at specified destinations. The project was completed virtually using the Turtlebot 3 Waffle Pi robot simulated in a Gazebo environment. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/aps360.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Accent Classification Network</strong> <br>[Final Project], University of Toronto, APS360 Applied Fundamentals of Machine Learning. With Mingshi C., Catherine G., Rocco R. <br> <a href="assets/pdf/APS360_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> Motivated by challenges experienced with voice-controlled devices, our team developed a neural network speech accent classifier which takes an accented English phrase as input and correctly identifies the origin of that speaker’s accent. The network developed was a CRNN architecture with mixed convolutional and recurrent neural network layers which learned both temporal and characteristic features of the audio data. The audio was processed into Mel-frequency cepstral coefficients (MFCC) features for input. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/ESC301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Electric Car Charger Robot </strong> <br>[Final Project], University of Toronto, ESC301 Praxis III. With Daniel R. and Chan Y. <br> <a href="assets/pdf/ESC301_Project%20Proposal%20Report.pdf">Detailed Design Report</a> <br> <br> Designed, conceptualized and prototyped an autonomous robot which locates and traverses to a charging port on a car to plug in the charger, utilizing time-of-flight sensor, color sensor, and pi-camera. Developed a modular program in Python and C++, each responsible for sensor or chassis control; utilized embedded programing</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Praxis2.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Sourdough IR Fermentometer Autonomous Robot </strong> <br> [Final Project], University of Toronto, ESC102 Praxis II. With Mingshi C., Daniela L., Peter S. <br> <a href="assets/pdf/ShowcasePoster.pdf">Poster</a> / <a href="assets/img/praxis2_demo.mp4">Demo Video</a> <br> <br> Designed a fully automated robotics system to inform sourdough bakers when their dough is ready to be put into the oven, in collaboration with local Toronto bakers at Forno Cultura and Blackbird Bakeries. The system develops a custom yeast-growth model and while the dough is expanding, the robot uses a 2 axis linear stage, time of flight sensor and temperature sensor to track the dough growth and alerts the bakers when it is ready. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Chess.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Chess AI</strong> <br> [Class Project], University of Toronto, CSC190 Algorithms and Data Structures. <br> <a href="https://github.com/JuliaChae/Chess-AI" rel="external nofollow noopener" target="_blank">Code</a> <br> <br>Developed an automatic chess player that while utilizes alpha-beta pruning on a min-max game tree in order to select the best move against a human opponent. </p1> </td> </tr> </tbody></table> &lt;/tbody&gt;&lt;/table&gt; <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Gameboy.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Original Gameboy Project: Bit's Adventures</strong> <br> [Final Project], University of Toronto, MIE438 Microcontrollers and Embedded Processors. <br> <a href="https://github.com/Maelstrum127/MIE438-GameBoy-Project" rel="external nofollow noopener" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=2vMHTOsuGYA&amp;ab_channel=CatGlossop" rel="external nofollow noopener" target="_blank">Demo Video</a> <br> <br> Bit’s Adventure is an original interactive RPG for the Original Game Boy which was coded using gbdk package in C. The game can be played on Game Boy emulators such as mGBA. </p1> </td> </tr> </tbody></table> <h2 style="padding-top: 30px;"><b>projects</b></h2> <p>For more projects, please visit my <a href="https://github.com/JuliaChae" rel="external nofollow noopener" target="_blank">Github</a>&lt;/b&gt;</p> <table style="padding-top: 10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/carla_imi.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Unsupervised Multimodal Representation Learning for Robot Controls</strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>. With Yewon L., Pranit C. <br> <br> Investigated how unsupervised representation learning could be utilized in an imitation learning (IL) task such as robot path following. In this project, we successfully trained a multimodal representation from unlabelled path traversal data, fine-tuned on minimal labeled data.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/multiview.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Analysis of Monocular 3D Object Detection Network Performance on Multiview Datasets</strong> <br> [Research Presentation] @ <strong><a href="https://www.trailab.utias.utoronto.ca/" rel="external nofollow noopener" target="_blank">TRAILab</a></strong>, funded by <strong>NSERC USRA</strong> <br> <a href="https://github.com/JuliaChae/M3D-RPN-Waymo" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Designed and executed an analysis of effect of camera perspectives on object detection networks, identifying limitations of multi-view self-driving datasets and current state-of-art monocular networks. The performance of trained monocular networks on various views of multiview dataset was analyzed with respect to multiple factors including distance, rotation and level of occlusion.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/pointfusion.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Adversarial Examples for Multimodal Object Detection Networks </strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>, funded by <strong>ESROP Dr Allison Mackay Award</strong> <br> <a href="https://github.com/JuliaChae/Pointfusion" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Led the implementation of multi-sensor object detection neural networks to state-of-art performance using PyTorch for an adversarial examples project, to improve robustness of image and lidar perception models. Trained fasterRCNN on nuScenes self driving car dataset and implemented Pointfusion architecture based on the original paper.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/RSX.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Rover Drive System</strong> <br> [Competition] University/European/Canadian Rover Challenges <br> <a href="https://github.com/rsx-utoronto/rover/tree/develop" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Spearheaded development of rover drive control software pipeline which includes joystick integration, configuration of I2C protocol for motor drivers, variable speed control and AR tag detection. Languages used were Python, C++ and ROS was utilized to handle communication between topics.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/rob301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Robot Mail Delivery Localization System</strong> <br>[Final Project], University of Toronto, ROB301 Introduction to Robotics. With Samantha U. <br> <a href="assets/pdf/ROB301_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> A PID control system, bayesian localization and state measurement models were developed in order to enable the robot to navigate an unknown hallway to deliver mail at specified destinations. The project was completed virtually using the Turtlebot 3 Waffle Pi robot simulated in a Gazebo environment. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/aps360.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Accent Classification Network</strong> <br>[Final Project], University of Toronto, APS360 Applied Fundamentals of Machine Learning. With Mingshi C., Catherine G., Rocco R. <br> <a href="assets/pdf/APS360_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> Motivated by challenges experienced with voice-controlled devices, our team developed a neural network speech accent classifier which takes an accented English phrase as input and correctly identifies the origin of that speaker’s accent. The network developed was a CRNN architecture with mixed convolutional and recurrent neural network layers which learned both temporal and characteristic features of the audio data. The audio was processed into Mel-frequency cepstral coefficients (MFCC) features for input. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/ESC301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Electric Car Charger Robot </strong> <br>[Final Project], University of Toronto, ESC301 Praxis III. With Daniel R. and Chan Y. <br> <a href="assets/pdf/ESC301_Project%20Proposal%20Report.pdf">Detailed Design Report</a> <br> <br> Designed, conceptualized and prototyped an autonomous robot which locates and traverses to a charging port on a car to plug in the charger, utilizing time-of-flight sensor, color sensor, and pi-camera. Developed a modular program in Python and C++, each responsible for sensor or chassis control; utilized embedded programing</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Praxis2.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Sourdough IR Fermentometer Autonomous Robot </strong> <br> [Final Project], University of Toronto, ESC102 Praxis II. With Mingshi C., Daniela L., Peter S. <br> <a href="assets/pdf/ShowcasePoster.pdf">Poster</a> / <a href="assets/img/praxis2_demo.mp4">Demo Video</a> <br> <br> Designed a fully automated robotics system to inform sourdough bakers when their dough is ready to be put into the oven, in collaboration with local Toronto bakers at Forno Cultura and Blackbird Bakeries. The system develops a custom yeast-growth model and while the dough is expanding, the robot uses a 2 axis linear stage, time of flight sensor and temperature sensor to track the dough growth and alerts the bakers when it is ready. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Chess.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Chess AI</strong> <br> [Class Project], University of Toronto, CSC190 Algorithms and Data Structures. <br> <a href="https://github.com/JuliaChae/Chess-AI" rel="external nofollow noopener" target="_blank">Code</a> <br> <br>Developed an automatic chess player that while utilizes alpha-beta pruning on a min-max game tree in order to select the best move against a human opponent. </p1> </td> </tr> </tbody></table> &lt;/tbody&gt;&lt;/table&gt; <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Gameboy.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Original Gameboy Project: Bit's Adventures</strong> <br> [Final Project], University of Toronto, MIE438 Microcontrollers and Embedded Processors. <br> <a href="https://github.com/Maelstrum127/MIE438-GameBoy-Project" rel="external nofollow noopener" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=2vMHTOsuGYA&amp;ab_channel=CatGlossop" rel="external nofollow noopener" target="_blank">Demo Video</a> <br> <br> Bit’s Adventure is an original interactive RPG for the Original Game Boy which was coded using gbdk package in C. The game can be played on Game Boy emulators such as mGBA. </p1> </td> </tr> </tbody></table> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Julia Chae. Last updated: April 27, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>