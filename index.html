<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Julia Chae</title> <meta name="author" content="Julia Chae"> <meta name="description" content="PhD Student at Massachusetts Institute of Technology "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://juliachae.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="#about"> about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="#education"> education </a> </li> <li class="nav-item "> <a class="nav-link" href="#publications"> publications </a> </li> <li class="nav-item "> <a class="nav-link" href="#projects"> projects </a> </li> <li class="nav-item "> <a class="nav-link" href="#misc"> interests </a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post" id="about"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Julia</span> Chae </h1> <p class="desc"><a href="https://www.eecs.mit.edu/" rel="external nofollow noopener" target="_blank">Massachusetts Institute of Technology</a>. Computer Vision and Machine Learning</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/headshot-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/headshot-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/headshot-1400.webp"></source> <img src="/assets/img/headshot.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="headshot.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am a EECS PhD student at <a href="https://www.csail.mit.edu/" rel="external nofollow noopener" target="_blank"><strong>MIT CSAIL</strong></a> advised by <a href="https://beerys.github.io/" rel="external nofollow noopener" target="_blank"><strong>Sara Beery</strong></a>. My PhD has been generously supported by the NSERC-PGS-D and MIT Andrew and Erna Viterbi Graduate Fellowship.</p> <p>My research focuses on bridging <strong>general-purpose</strong> models with <strong>expert-level intelligence</strong>. I am interested in how evolving <strong>domain knowledge</strong> can be integrated into large AI models — through curated <strong><em>synthetic data</em></strong>, <strong><em>targeted self-supervision</em></strong>, and <strong><em>expert feedback</em></strong> — while preserving their adaptability and generalization. More broadly, I aim to scientifically understand how to overcome real-world data gaps, guide scalable and robust specialization, and design models that, like humans, refine their expertise in alignment with the structure of complex real-world environments.</p> <p>I’m particularly interested in applications to <strong>ecology</strong> and <strong>biodiversity monitoring</strong> - previously, my work has focused on robotics applications.</p> <p>Prior to my PhD, I received by BASc at the <strong>University of Toronto</strong> in <a href="https://discover.engineering.utoronto.ca/programs/engineering-programs/engineering-science/" rel="external nofollow noopener" target="_blank"><strong>Engineering Science</strong></a>, majoring in Robotics with a minor in <a href="https://undergrad.engineering.utoronto.ca/academics-registration/minors-certificates/undergraduate-engineering-minors/minor-in-artificial-intelligence/" rel="external nofollow noopener" target="_blank">Machine Intelligence</a>. At UofT, I worked with Prof. <a href="https://www.cs.utoronto.ca/~fidler/" rel="external nofollow noopener" target="_blank">Sanja Fidler</a> (affiliations: <a href="https://learning.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">UofT Machine Learning</a>, <a href="https://vectorinstitute.ai/" rel="external nofollow noopener" target="_blank">Vector Institute</a>, <a href="https://nv-tlabs.github.io/" rel="external nofollow noopener" target="_blank">NVIDIA</a>) on <strong>self-supervised representation learning</strong>, and with Prof. <a href="http://www.cs.toronto.edu/~florian/" rel="external nofollow noopener" target="_blank">Florian Shkurti</a> (affiliations: <a href="https://vectorinstitute.ai/" rel="external nofollow noopener" target="_blank">Vector Institute</a>, <a href="https://robotics.utoronto.ca/" rel="external nofollow noopener" target="_blank">UofT Robotics Institute</a>), Prof. <a href="https://www.trailab.utias.utoronto.ca/stevenwaslander" rel="external nofollow noopener" target="_blank">Steven Waslander</a> (affiliations: <a href="https://www.utias.utoronto.ca/" rel="external nofollow noopener" target="_blank">UTIAS</a>, U of T Robotics Institute) on <strong>robot learning and perception</strong>. Please see my CV below for more details.</p> <p>If you would like to chat, feel free to reach out to me via one of the contact information below.</p> </div> <div class="custom_projects" id="research_interests"> <h2><b>research interests</b></h2> <ul> <li>Efficient adaptation of multimodal models for domain specialization and personalized applications</li> <li>Targeted synthetic data generation to fill data gaps for enhancing robustness and generalization</li> <li>Self-supervised and few-shot learning</li> </ul> </div> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Apr 22, 2025</th> <td> Attending ICLR in Singapore to present <a href="https://personalized-rep.github.io/" rel="external nofollow noopener" target="_blank">Personalized Representation from Personalized Generation</a> at the main conference! </td> </tr> <tr> <th scope="row">Mar 5, 2025</th> <td> Gave a talk at Cohere (See recording <a href="https://www.youtube.com/watch?v=vPBzjhdTTD8&amp;list=PLLalUvky4CLJKDaiWCumhsJpHNDhZeVll&amp;index=3&amp;t=1s" rel="external nofollow noopener" target="_blank">here</a>) </td> </tr> <tr> <th scope="row">Jan 22, 2025</th> <td> Our paper, <a href="https://arxiv.org/abs/2412.16156" rel="external nofollow noopener" target="_blank">Personalized Representation from Personalized Generation</a> was accepted to ICLR 2025! </td> </tr> <tr> <th scope="row">Jan 6, 2025</th> <td> Served as an instructor for the three-week intensive <a href="https://cv4ecology.caltech.edu/" rel="external nofollow noopener" target="_blank">Computer Vision for Ecology Workshop</a> at Caltech Resnick Sustainability Institute </td> </tr> <tr> <th scope="row">Sep 30, 2024</th> <td> Co-Organized 1st ever <a href="https://cv4e.netlify.app/" rel="external nofollow noopener" target="_blank">CV4E Workshop</a> at ECCV in Milan! </td> </tr> </table> </div> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%63%68%61%65%6E%61%79%6F@%6D%69%74.%65%64%75"><i class="fas fa-envelope"></i></a> <a href="https://github.com/JuliaChae" target="_blank" title="GitHub" rel="external nofollow noopener"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/julia-chae" target="_blank" title="LinkedIn" rel="external nofollow noopener"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/juliachae_" target="_blank" title="Twitter" rel="external nofollow noopener"><i class="fab fa-twitter"></i></a> <a href="assets/pdf/JuliaChae_CV_Jan25.pdf" style="color:black"><i class="fas fa-file-alt"></i></a> </div> <div class="custom_projects" id="education"> <h2 style="padding-top: 30px;"><b>education</b></h2> <table style="padding-top: 10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/mit.png" alt="clean-usnob" width="140" height="140"> </td> <td width="75%" valign="middle"> <p> <strong>Massachusetts Institute of Technology</strong></p> <p1>PhD in Computer Science | 2023 - Present <br><strong>Affiliations:</strong> Computer Science and Artificial Intelligence Lab (<a href="https://www.csail.mit.edu/" rel="external nofollow noopener" target="_blank">CSAIL</a>), Embodied Intelligence, <a href="https://beerys.github.io/" rel="external nofollow noopener" target="_blank">BeeryLab</a> </p1> </td> </tr> </tbody></table> <table style="padding-top: 20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/uoft.jpg" alt="clean-usnob" width="140" height="140"> </td> <td width="75%" valign="middle"> <p> <strong>University of Toronto</strong></p> <p1><a href="https://engsci.utoronto.ca/program/what-is-engsci/" rel="external nofollow noopener" target="_blank">BASc. in Engineering Science</a> | 2018 - 2023 <br><strong>Robotics Major, Machine Intelligence Minor</strong> (cGPA: 3.97/4.0) </p1> </td> </tr> </tbody></table> </div> <div class="custom_projects" id="publications"> <h2 style="padding-top: 30px;"><b>publications</b></h2> <table style="padding-top: 10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/prpg_teaser.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Personalized Representation from Personalized Generation</strong> <br> Shobhita Sundaram*, <strong> Julia Chae* </strong>, Yonglong Tian, Sara Beery, Phillip Isola <br><em><a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">International Conference on Learning Representations (ICLR)</a></em>, 2025 <br> <a href="https://personalized-rep.github.io/" rel="external nofollow noopener" target="_blank">Project Page</a> / <a href="https://arxiv.org/abs/2412.16156" rel="external nofollow noopener" target="_blank">Paper</a> / <a href="https://github.com/ssundaram21/personalized-rep" rel="external nofollow noopener" target="_blank">Code</a> / <a href="https://huggingface.co/datasets/chaenayo/PODS" rel="external nofollow noopener" target="_blank">PODS Dataset</a> <br><small><em>* Denotes equal contribution</em></small> <br> <br> We introduce a representation learning framework that leverages few-shot T2I diffusion–generated synthetic images to learn object-centric representations for diverse downstream tasks. Using reformulations of two benchmarks and a novel personalized dataset, we demonstrate significant gains across various downstream vision tasks, and analyze the factors driving these improvements.</p1> </td> </tr> </tbody></table> <table style="padding-top: 10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/epson.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>SKP: Semantic 3D Keypoint Detection for Category-Level Robotic Manipulation</strong> <br> Zhongzhen Luo , Wenjie Xue, <strong> Julia Chae </strong>, Guoyi Fu <br><em><a href="https://www.ieee-ras.org/publications/ra-l" rel="external nofollow noopener" target="_blank">IEEE Robotics and Automation Letters</a></em>, April 2022 <br> <a href="https://ieeexplore.ieee.org/document/9730091/" rel="external nofollow noopener" target="_blank">Journal Paper</a> <br> <br> SKP is a multi-task perception module which jointly optimizes keypoint and part-segmentation learning in order to improve generalizability of category-level objects for robotic grasping. This model demonstrates accurate performance in real data, while being trained purely in the synthetic domain. </p1> </td> </tr> </tbody></table> <table style="padding-top: 20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/CaDDN.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Categorical Depth Distribution Network for Monocular 3D Object Detection</strong> <br> Cody Reading, Ali Harakeh, <strong> Julia Chae </strong>, Steven L. Waslander <br>[Oral Presentation]. <em><a href="http://cvpr2021.thecvf.com/" rel="external nofollow noopener" target="_blank">Conference on Computer Vision and Pattern Recognition</a></em>, 2021 <br> <a href="https://trailab.github.io/CaDDN/" rel="external nofollow noopener" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/2103.01100.pdf" rel="external nofollow noopener" target="_blank">Paper</a> / <a href="https://github.com/TRAILab/CaDDN" rel="external nofollow noopener" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=E3NoO_c6tPg&amp;t=5s&amp;ab_channel=trailab" rel="external nofollow noopener" target="_blank">Talk</a> <br> <br> CaDDN is an approach for joint depth estimation and object detection that uses a predicted categorical depth distribution for each pixel to project contextual feature information in 3D space. CaDDN ranked 1st amongst published monocular methods on KITTI and Waymo 3D object detection datasets at publication.</p1> </td> </tr> </tbody></table> <h2 style="padding-top: 30px;"><b>theses</b></h2> <table style="padding-top: 10px; width:100%;border:0px;border-spacing:0px;border-collapse:separate;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/selfsup.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Self-supervised Dense Representation Learning </strong> <strong> Julia Chae </strong> <br> [BASc Thesis] @ <strong><a href="https://engsci.utoronto.ca/program/thesis/" rel="external nofollow noopener" target="_blank">U of T Eng Sci</a></strong>, supervised by <strong><a href="https://www.trailab.utias.utoronto.ca/" rel="external nofollow noopener" target="_blank">Sanja Fidler</a></strong> <br> <br> Thesis on the exploration of inter-image relationships for contrastive learning to investigate how context between images can be used to improve part-based dense representation learning. </p1> <br> <br> </td> </tr> </tbody></table> </div> <div class="custom_projects" id="projects"> <h2 style="padding-top: 30px;"><b>projects</b></h2> <p>For more projects, please visit my <a href="https://github.com/JuliaChae" rel="external nofollow noopener" target="_blank">Github</a></p> <table style="padding-top: 10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/carla_imi.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Unsupervised Multimodal Representation Learning for Robot Controls</strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>. With Yewon L., Pranit C. <br> <br> Investigated how unsupervised representation learning could be utilized in an imitation learning (IL) task such as robot path following. In this project, we successfully trained a multimodal representation from unlabelled path traversal data, fine-tuned on minimal labeled data.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/multiview.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Analysis of Monocular 3D Object Detection Network Performance on Multiview Datasets</strong> <br> [Research Presentation] @ <strong><a href="https://www.trailab.utias.utoronto.ca/" rel="external nofollow noopener" target="_blank">TRAILab</a></strong>, funded by <strong>NSERC USRA</strong> <br> <a href="https://github.com/JuliaChae/M3D-RPN-Waymo" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Designed and executed an analysis of effect of camera perspectives on object detection networks, identifying limitations of multi-view self-driving datasets and current state-of-art monocular networks. The performance of trained monocular networks on various views of multiview dataset was analyzed with respect to multiple factors including distance, rotation and level of occlusion.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/pointfusion.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Adversarial Examples for Multimodal Object Detection Networks </strong> <br> [Research Project] @ <strong><a href="https://rvl.cs.toronto.edu/" rel="external nofollow noopener" target="_blank">RVL</a></strong>, funded by <strong>ESROP Dr Allison Mackay Award</strong> <br> <a href="https://github.com/JuliaChae/Pointfusion" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Led the implementation of multi-sensor object detection neural networks to state-of-art performance using PyTorch for an adversarial examples project, to improve robustness of image and lidar perception models. Trained fasterRCNN on nuScenes self driving car dataset and implemented Pointfusion architecture based on the original paper.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/RSX.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Rover Drive System</strong> <br> [Competition] University/European/Canadian Rover Challenges <br> <a href="https://github.com/rsx-utoronto/rover/tree/develop" rel="external nofollow noopener" target="_blank">Code</a> <br> <br> Spearheaded development of rover drive control software pipeline which includes joystick integration, configuration of I2C protocol for motor drivers, variable speed control and AR tag detection. Languages used were Python, C++ and ROS was utilized to handle communication between topics.</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/rob301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Robot Mail Delivery Localization System</strong> <br>[Final Project], University of Toronto, ROB301 Introduction to Robotics. With Samantha U. <br> <a href="assets/pdf/ROB301_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> A PID control system, bayesian localization and state measurement models were developed in order to enable the robot to navigate an unknown hallway to deliver mail at specified destinations. The project was completed virtually using the Turtlebot 3 Waffle Pi robot simulated in a Gazebo environment. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/aps360.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Accent Classification Network</strong> <br>[Final Project], University of Toronto, APS360 Applied Fundamentals of Machine Learning. With Mingshi C., Catherine G., Rocco R. <br> <a href="assets/pdf/APS360_FinalReport.pdf">Detailed Report</a> / Code Coming Soon <br> <br> Motivated by challenges experienced with voice-controlled devices, our team developed a neural network speech accent classifier which takes an accented English phrase as input and correctly identifies the origin of that speaker’s accent. The network developed was a CRNN architecture with mixed convolutional and recurrent neural network layers which learned both temporal and characteristic features of the audio data. The audio was processed into Mel-frequency cepstral coefficients (MFCC) features for input. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/ESC301.jpg" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Autonomous Electric Car Charger Robot </strong> <br>[Final Project], University of Toronto, ESC301 Praxis III. With Daniel R. and Chan Y. <br> <a href="assets/pdf/ESC301_Project%20Proposal%20Report.pdf">Detailed Design Report</a> <br> <br> Designed, conceptualized and prototyped an autonomous robot which locates and traverses to a charging port on a car to plug in the charger, utilizing time-of-flight sensor, color sensor, and pi-camera. Developed a modular program in Python and C++, each responsible for sensor or chassis control; utilized embedded programing</p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Praxis2.JPG" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Sourdough IR Fermentometer Autonomous Robot </strong> <br> [Final Project], University of Toronto, ESC102 Praxis II. With Mingshi C., Daniela L., Peter S. <br> <a href="assets/pdf/ShowcasePoster.pdf">Poster</a> / <a href="assets/img/praxis2_demo.mp4">Demo Video</a> <br> <br> Designed a fully automated robotics system to inform sourdough bakers when their dough is ready to be put into the oven, in collaboration with local Toronto bakers at Forno Cultura and Blackbird Bakeries. The system develops a custom yeast-growth model and while the dough is expanding, the robot uses a 2 axis linear stage, time of flight sensor and temperature sensor to track the dough growth and alerts the bakers when it is ready. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Chess.png" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Chess AI</strong> <br> [Class Project], University of Toronto, CSC190 Algorithms and Data Structures. <br> <a href="https://github.com/JuliaChae/Chess-AI" rel="external nofollow noopener" target="_blank">Code</a> <br> <br>Developed an automatic chess player that while utilizes alpha-beta pruning on a min-max game tree in order to select the best move against a human opponent. </p1> </td> </tr> </tbody></table> <table style="padding-top: 30px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> <tr> <td style="padding-right: 20px; width:25%;vertical-align:middle"> <img src="assets/img/Gameboy.gif" alt="clean-usnob" width="160" height="140"> </td> <td width="75%" valign="middle"> <p1> <strong>Original Gameboy Project: Bit's Adventures</strong> <br> [Final Project], University of Toronto, MIE438 Microcontrollers and Embedded Processors. <br> <a href="https://github.com/Maelstrum127/MIE438-GameBoy-Project" rel="external nofollow noopener" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=2vMHTOsuGYA&amp;ab_channel=CatGlossop" rel="external nofollow noopener" target="_blank">Demo Video</a> <br> <br> Bit’s Adventure is an original interactive RPG for the Original Game Boy which was coded using gbdk package in C. The game can be played on Game Boy emulators such as mGBA. </p1> </td> </tr> </tbody></table> </div> <div class="misc" id="misc"> <div style="padding-top: 20px;"> <h2><b>interests/ hobbies</b></h2> </div> <div class="row" style="padding-top: 20px;"> <div class="col-md-4 portfolio-item" align-items="center"> <img class="img-responsive" src="assets/img/piano.jpg" width="230" height="230" halign="middle"> <br><p1 style="padding-top: 10px;"><strong>Music Performance and Enthusiast</strong> <br><br>I have an <a href="https://www.rcmusic.com/learning/examinations/recognizing-achievement/arct-lrcm" rel="external nofollow noopener" target="_blank">Associate Diploma</a> in piano performance and I enjoy playing for leisure and at community events. I also enjoy listening to music of various genre including but not limited to classic and alternative rock, electronic pop and R&amp;B</p1> </div> <div class="col-md-4 portfolio-item"> <img class="img-responsive" src="assets/img/juneau.jpg" width="230" height="230"> <br><p1 style="padding-top: 10px;"><strong>Hiking</strong> <br><br>I'm lucky to have grown up in Toronto which has an abundance of parks and nature trails to explore. I enjoy finding new spots on the weekends and observing wildlife. Recently I encountered a fox family, American mink and a beaver! </p1> </div> <div class="col-md-4 portfolio-item"> <img class="img-responsive" src="assets/img/chaetime.JPG" width="230" height="230"> <br><p1 style="padding-top: 10px;"><strong>Cooking</strong> <br><br>I'm an amateur cook who enjoys learning dishes from all over the world! I run a small cooking page (pictured above) where I share my journey with my friends, family and other members of the cooking community. If you have any recipe recommendations please let me know</p1> </div> </div> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Julia Chae. Last updated: April 27, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>